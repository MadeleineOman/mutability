{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5de05c8d-7c31-44cf-81a4-0fb4ef38f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp ../../../data/liver/objects/model6/tracksColDict.txt ../../../data/liver/objects/model9/\n",
    "#! cp ../../../data/skin/objects/model6/tracksColDict.txt ../../../data/skin/objects/model9/\n",
    "#! cp ../../../data/germline/objects/model6/tracksColDict.txt ../../../data/germline/objects/model9/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08486aea-48b3-4c97-ae4f-c6597bdc02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from numpy.random import choice\n",
    "import collections\n",
    "from Bio import AlignIO\n",
    "import pysam \n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import multiprocessing\n",
    "import sys \n",
    "import json \n",
    "import time # for timing the loop \n",
    "#home-made modules : \n",
    "sys.path.append('/research/projects/hsapiens/mutability/analysis/global/track_data/annotation/') \n",
    "import annotation_handling\n",
    "sys.path.append('/research/projects/hsapiens/mutability/analysis/modules/closest_value/') \n",
    "import closest_val\n",
    "\n",
    "\n",
    "tmp_file_dir =\"../../../\"\n",
    "\n",
    "# command line input ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# tissue = sys.argv[1]\n",
    "# model_name = sys.argv[2]\n",
    "# list_of_surrounding_contexts = json.loads(sys.argv[3]) #note if you chnagee/ increase this, then you oncrese the buffer zone (not using sites in the buffer, len(dna)-max_distance )\n",
    "# if \"_allTissueSpecTracks\" in sys.argv: allTissueSpecTracks = True\n",
    "# else: allTissueSpecTracks = False \n",
    "\n",
    "tissue = \"liver\"\n",
    "model_name = \"model9\" \n",
    "list_of_surrounding_contexts = [0,100,10000]\n",
    "allTissueSpecTracks = False\n",
    "\n",
    "model_desc=\"\"\n",
    "\n",
    "if tissue == \"germline\": \n",
    "    mutations_lines = open(tmp_file_dir+'data/germline/mutation_data/mutations_hg18_final.bed').readlines()\n",
    "    mutations_df = pd.read_table(tmp_file_dir+'data/germline/mutation_data/mutations_hg18_final.bed',sep=\"\\t\",header = None)\n",
    "    mutations_df.columns = [\"chromosome\",\"start\",\"fake_end\",\"ref\",\"alt\",\"Fathers_age_at_conception\",\"Mothers_age_at_conception\"]\n",
    "elif tissue in [\"blood\",\"liver\"]:\n",
    "    mutations_lines = open(tmp_file_dir+'data/{t}/mutations/mutations.bed'.format(t=tissue)).readlines()\n",
    "    mutations_df = pd.read_table(tmp_file_dir+'data/{t}/mutations/mutations.bed'.format(t=tissue),sep=\"\\t\",header = None)\n",
    "    mutations_df.columns = [\"chromosome\",\"start\",\"fake_end\",\"ref\",\"alt\",\"ID\",\"VAF\",\"Gene name\", \"Region\", \"AA\", \"COSMIC\", \"Species\", \"Gender\", \"Age_in_years\",           \n",
    "                            \"Tissue/Cell type\",\"Single-cell_genomics_biotechnology_or_Method\",\"Control_sample_or_tissue\"]\n",
    "elif tissue == \"skin\": \n",
    "    mutations_lines = open(tmp_file_dir+'data/skin/mutations/SomaMutDB/mutations_0based.bed'.format(t=tissue)).readlines()\n",
    "    mutations_df = pd.read_table(tmp_file_dir+'data/skin/mutations/SomaMutDB/mutations_0based.bed'.format(t=tissue),sep=\"\\t\",header = None)\n",
    "    mutations_df.columns = [\"chromosome\",\"start\",\"fake_end\",\"ref\",\"alt\"]+list(mutations_df.columns)[5:]\n",
    "else: \n",
    "    print(tissue,\" tissue specified not yet supported  !!~~~!!!~~~~!!\")\n",
    "\n",
    "    \n",
    "\n",
    "#dictionry where i specify which col contains the information in the datafile , 0 indexed \n",
    "tracksColFile_dict = json.load(open(tmp_file_dir+\"data/{t}/objects/{m}/tracksColDict.txt\".format(m=model_name,t=tissue)))#  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63c196bc-5048-400f-86f2-6e820101566b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liver : fill each chromosome's empty list  with the sites for that chrom\n",
      "liver make the sites list with the chr# and site\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if allTissueSpecTracks == True: \n",
    "    model_desc=\"_allTissueSpecTracks\"\n",
    "    tissue_specific_tracknames = [\"H3k27\",'H3k4me3',\"H3k4me1\",\"H3k27me3\",\"H3k36me3\",\"DNAse\",\"Transcription\",\"methylation\"]\n",
    "\n",
    "    for track in tissue_specific_tracknames: \n",
    "        if tissue==\"liver\" and track==\"H3k27me3\":  continue\n",
    "        tracksColFile_dict[track+\"_\"+tissue] = tracksColFile_dict[track]\n",
    "        del tracksColFile_dict[track]\n",
    "\n",
    "    other_tissues = [\"blood\",\"skin\",\"liver\",\"germline\"]\n",
    "    other_tissues.remove(tissue)\n",
    "\n",
    "    for other_tissue in other_tissues: \n",
    "        other_tracksColDict = json.load(open(tmp_file_dir+\"data/{t}/objects/{m}/tracksColDict.txt\".format(m=model_name,t=other_tissue)))\n",
    "        for track in tissue_specific_tracknames: \n",
    "            if not (other_tissue==\"liver\" and track==\"H3k27me3\"):\n",
    "                tracksColFile_dict[track+\"_\"+other_tissue] = other_tracksColDict[track]\n",
    "                \n",
    "\n",
    "\n",
    "#mutant sites ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "#create a dictionary where each chrom key will have a n empty list \n",
    "duplicate_lines = []\n",
    "muts_bychrom_dict = {}\n",
    "for x in range(1,23): \n",
    "    key_string = 'chr{n}'.format(n=x)\n",
    "    muts_bychrom_dict[key_string] = []\n",
    "    \n",
    "print(tissue,\": fill each chromosome's empty list  with the sites for that chrom\")\n",
    "non_chrnMuts = []#create list of chrom names that dont belong to chrN format --> disgnostic \n",
    "for line in (mutations_lines[1:]): \n",
    "    if line[0]==\"c\":                                               #aking sure the line is a chrN (lots of weird junk..) \n",
    "        chrom_mut = line.split(\"\\t\")[0]\n",
    "        mut_startSite = line.split(\"\\t\")[1]                  #getting rid of the weird double(hgopefully) \n",
    "        if chrom_mut in muts_bychrom_dict.keys():                  #controlling for chrX/chrY\n",
    "            if mut_startSite not in muts_bychrom_dict[chrom_mut]: \n",
    "                muts_bychrom_dict[chrom_mut].append(mut_startSite)\n",
    "            else: duplicate_lines.append(line)\n",
    "        else: \n",
    "            non_chrnMuts.append(chrom_mut)\n",
    "\n",
    "#testing making usre the only sites that dont make it are sex chromosome mutations \n",
    "timestamp = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "error_log = str(\"df created on \"+timestamp+\"\\n\")\n",
    "error_log+=(str(len(non_chrnMuts))+\"  non chrN muts (ommited) from these lables: \"+str(list(np.unique(non_chrnMuts)))+\"\\n\")\n",
    "\n",
    "#add the sites infro from file \n",
    "sites = []#sites = list of sites \n",
    "for chrom_key in muts_bychrom_dict.keys(): \n",
    "    for mutation_element in muts_bychrom_dict[chrom_key]: \n",
    "        sites.append([chrom_key, int(mutation_element),1]) #the 1 is for mutation status column. 1 = yes \n",
    "\n",
    "\n",
    "#non -mutant sites ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "#get chrom length information so I can perform weighted choice for non-mut site selection\"\n",
    "ChromLengths = pd.read_csv(tmp_file_dir+'data/global/sequence/hg38_chromosomelengths.csv') #read in the csv file of hg38 chrom lengths I found on the internets \n",
    "total_length=0 #lets sum (get the total length) \n",
    "for length in list(ChromLengths.Length): \n",
    "    total_length+=int(length.replace(\",\",\"\"))\n",
    "\n",
    "#build dictionary to store porbability \n",
    "dict_lengths = {}#creat emepty dictionary \n",
    "for x in range (0,22): \n",
    "        tmp_index = x +1\n",
    "        length = str(ChromLengths[x:x+1]).split()[4]\n",
    "        length = length.replace(\",\", \"\")\n",
    "        length = int(length)\n",
    "        dict_lengths[\"chr\"+str(tmp_index)] = length\n",
    "\n",
    "#make the porbability of choosing a chrom based on length \n",
    "list_chroms = ['chr' + str(i) for i in range(1, 23)]\n",
    "list_chrom_probabilities = []\n",
    "for chrom in list_chroms: \n",
    "    list_chrom_probabilities.append(dict_lengths[chrom]/total_length)\n",
    "list_chrom_probabilities[0] = list_chrom_probabilities[0]+1-sum(list_chrom_probabilities) # adds the 0.00000001 left from rounding errors to the chr1 so sum adds perfectly to 1. \n",
    "assert(sum(list_chrom_probabilities)==1)\n",
    "\n",
    "#perfrom the non-mutant site draw \n",
    "number_nonmuts = int(len(sites)*1.2)\n",
    "chrom_draw = choice(list_chroms, number_nonmuts,p=list_chrom_probabilities)\n",
    "\n",
    "print(tissue,\"make the sites list with the chr# and site\" )\n",
    "for i in (range(1,23)): \n",
    "    chrom = \"chr\"+str(i)\n",
    "    chrom_nchoose = list(chrom_draw).count(\"chr\"+str(i))\n",
    "    chrom_sites_chosen = random.sample(range(1, dict_lengths[chrom]), chrom_nchoose) #without duplucates \n",
    "    for j in chrom_sites_chosen: \n",
    "        sites.append([chrom,j,0])# the 0 if for the mutation status column. 0 = no \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "357dce5a-7e35-4c7f-b6f0-dd7a701a9025",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liver making the fastas dictionary\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# genral declarations before the big function ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "distance_max = max(list_of_surrounding_contexts)\n",
    "\n",
    "fastas_dict = {}   # creating dictionary with fasta alignment, length of seq, \n",
    "print(tissue,\"making the fastas dictionary\")\n",
    "for chrom in (list_chroms):\n",
    "    filename_tmp = tmp_file_dir+\"data/global/sequence/{c}.fa.gz\".format(c=chrom)\n",
    "    fastas_dict[chrom] = []\n",
    "    with gzip.open(filename_tmp, \"rt\") as handle:\n",
    "        fastas_dict[chrom].append(AlignIO.read(handle,\"fasta\"))\n",
    "        alignment_tmp = fastas_dict[chrom][0]\n",
    "        fastas_dict[chrom].append(len(str(alignment_tmp[0].seq)))\n",
    "\n",
    "                 \n",
    "#generate header ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~        \n",
    "\n",
    "header = \"Chromosome\"+ \"\\t\"+\"site\"+\"\\t\" +\"triplet\"+\"\\t\"+\"mutation_status\"                        # creating the begining of the header \n",
    "for trackname in tracksColFile_dict.keys():   # the rest of the header is a function of tracks \n",
    "    if trackname in [\"annotation\",\"mappability\",\"dist_rep_org_main\",\"dist_rep_org_all\",\"CpGisland\"]: \n",
    "        header = header + \"\\t\"+str(trackname)\n",
    "    elif  \"methylation\" in trackname:\n",
    "        for distance in list_of_surrounding_contexts: \n",
    "            header = header + \"\\t\"+str(trackname)+\"_precent\"+\"-\"+str(distance)+ \"\\t\"+str(trackname)+\"_coverage\"+\"-\"+str(distance)\n",
    "    else: \n",
    "        for distance in list_of_surrounding_contexts:                                                # and distance (need a col for every track and for every distance value within ) \n",
    "            header = header + \"\\t\"+str(trackname)+\"-\"+str(distance)\n",
    "for distance in list_of_surrounding_contexts:                                                    # creating the end of the header assoc with no track (the seqeunce at different \n",
    "    header = header + \"\\t\"+\"Apercent-\"+str(distance)+ \"\\t\"+\"Gpercent-\"+str(distance)+ \"\\t\"+\"Cpercent-\"+str(distance)+ \"\\t\"+\"Tpercent-\"+str(distance)   # distace values) \n",
    "header = header +\"\\n\"                                                                            # obviously needs to end with a \\n \n",
    "\n",
    "\n",
    "filename_df = tmp_file_dir+'data/{a}/dataframes/{m}/predictorDf{d}.txt'.format(a=tissue,m=model_name,d=model_desc)\n",
    "with open(filename_df,\"w\") as f: \n",
    "    f.write(header)     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab975140-1c24-421c-acc5-6b48d93d8375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "#big boy function \n",
    "def predictor_rowString(site): \n",
    "    row = []\n",
    "    if site[1] <= distance_max or site[1]+distance_max >= fastas_dict[site[0]][1]:               # only use sites that will have values for site +- max distance (buffer). second element in fastas dict is the length \n",
    "        row.extend([str(site),\"out of buffer range\"])\n",
    "\n",
    "    else: \n",
    "        row.extend([site[0], site[1]])\n",
    "        alignment = fastas_dict[site[0]][0]                                                       #create the alingment from the list of fastas \n",
    "\n",
    "        #makes the triplet and tests alignment \n",
    "        if site[2] ==1:                                     #muts_by-chrom_dict is literally a dictionary containing list of sites that are mutations in that chrom. \n",
    "            mutation_row = mutations_df[(mutations_df.chromosome == site[0]) & (mutations_df.start == site[1])]  #get the row containing mut info out of the df \n",
    "            old_bp = mutation_row.ref.values[0]\n",
    "            old_triplet = (str(alignment[0,site[1]-1])+str(old_bp)+str(alignment[0,site[1]+1])).upper()\n",
    "            row.extend([old_triplet, 1])#usin the old/ref triplet in the df instead (the mutation happened to the old triplet! )\n",
    "\n",
    "            seq_triplet = str(alignment[0,site[1]-1:site[1]+2].seq)\n",
    "            if old_triplet.upper() != seq_triplet.upper():                                                          #testing that \n",
    "                row.append(\"discordant. triplet using daata = \"+old_triplet+\", seqeunce triplet = \"+seq_triplet)\n",
    "        else: \n",
    "            triplet= str(alignment[0,site[1]-1:site[1]+2].seq).upper()\n",
    "            row.extend([triplet,0])\n",
    "\n",
    "        for trackname,track_val in tracksColFile_dict.items():    \n",
    "            data_col = track_val[0] \n",
    "            global_or_tissue_specific = track_val[1]\n",
    "            Na_is_0_or_NA = track_val[2]\n",
    "            filename = tmp_file_dir+track_val[3]\n",
    "\n",
    "            if trackname == \"annotation\": \n",
    "                if not [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1], site[1]+1)]:                #if no value at that site \n",
    "                     row.append(\"not_transcribed\")\n",
    "                else: \n",
    "                    track_output = [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1], site[1]+1)]\n",
    "                    old_labels = [element.split()[2] for element in track_output]\n",
    "                    converted_list, final_label,alien_labels = [],str(),[] \n",
    "                    for label in old_labels: \n",
    "                        if label in annotation_handling.annotation_conversion.keys():  #even though i though i controlled for it, occasiaonlyl there would eb anew label, so sontrol for this \n",
    "                            converted_list.append(annotation_handling.annotation_conversion[label])\n",
    "                        else: \n",
    "                            alien_labels.append(label)\n",
    "                    final_label = annotation_handling.annotation_priorityLabel(converted_list)\n",
    "                    if len(alien_labels) != 0:    # if there is an \"alien\" annotation label, then just add that into the position and i can handle on ind basis later \n",
    "                        for label in alien_labels: \n",
    "                            final_label+=\"_\"+label\n",
    "                    row.append(final_label)\n",
    "            elif trackname == \"mappability\": \n",
    "                if [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1]-distance, site[1]+distance+1)]: row.append(\"mappable\")\n",
    "                else: row.append(\"not\")\n",
    "            elif trackname in [\"dist_rep_org_main\",\"dist_rep_org_all\"]: \n",
    "                row.append(closest_val.shortest_distance(site,filename))\n",
    "            elif trackname == \"CpGisland\": \n",
    "                if [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1], site[1]+1)]: row.append(\"island\")\n",
    "                elif closest_val.shortest_distance(site,filename) <= 2000: row.append(\"shore\")\n",
    "                else: row.append(\"not\")\n",
    "            elif \"methylation\" in trackname:\n",
    "                #########################################working below \n",
    "                for distance in list_of_surrounding_contexts: \n",
    "                    track_output = [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1]-distance, site[1]+1+distance)]\n",
    "                    if track_output: \n",
    "                        num_reads_list,percent_methyl_list,next_door = [],[],False\n",
    "                        for record in track_output: \n",
    "                            cur_site = int(record.split(\"\\t\")[1])\n",
    "                            if cur_site == site[1]: \n",
    "                                num_reads_at_site,percent_methylated = record.split(\"|\")[6:8]\n",
    "                                if int(num_reads_at_site) >= 3: \n",
    "                                    num_reads_list.append(int(num_reads_at_site))\n",
    "                                    percent_methyl_list.append(float(percent_methylated))\n",
    "                            elif cur_site in [site[1]-1,site[1]+1] and distance in [0,1]: next_door = True\n",
    "                        if num_reads_list: # if the list has eleemtns \n",
    "                            row.extend([np.mean(percent_methyl_list),np.mean(num_reads_list)])\n",
    "                        else: # if the list is empty \n",
    "                            if next_door==True: #if the only reason the result is empty is that there is nothing htere, but tabix grabbed it b/c next door \n",
    "                                row.extend([\"no_percent_data\",\"no_coverage_data\"])\n",
    "                            else: #the result is empty because all the returns had coverage less than 3 \n",
    "                                row.extend([\"infsuff_coverage\",\"infsuff_coverage\"])\n",
    "                #         else: \n",
    "                #             for record in track_output:\n",
    "                #                 num_reads_at_site,percent_methylated = record.split(\"|\")[6:8]\n",
    "                #                 if int(num_reads_at_site) >= 3: \n",
    "                #                     num_reads_list.append(int(num_reads_at_site))\n",
    "                #                     percent_methyl_list.append(float(percent_methylated))\n",
    "                #             row.extend([np.mean(percent_methyl_list),np.mean(num_reads_list)])\n",
    "                    else: \n",
    "                        row.extend([\"no_percent_data\",\"no_coverage_data\"])\n",
    "            ############################################################################working abovce\n",
    "            else: \n",
    "                for distance in list_of_surrounding_contexts: \n",
    "                    if not [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1]-distance, site[1]+distance+1)]:                #if no value at that site \n",
    "                        if Na_is_0_or_NA == \"Na=0\": \n",
    "                            row.append(0)\n",
    "                        else: \n",
    "                            row.extend([\"NA\"])                                                                                      \n",
    "                    else:                                                   \n",
    "                        track_output = [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1]-distance, site[1]+distance+1)]\n",
    "                        multiple_values = []\n",
    "\n",
    "                        if tracksColFile_dict[trackname][0] == 4:\n",
    "                            for element in track_output: \n",
    "                                multiple_values.append(float(element.split()[4]))\n",
    "                            average_value = sum(multiple_values)/len(multiple_values) \n",
    "                            row.append(average_value)\n",
    "                        elif tracksColFile_dict[trackname][0] == 3: \n",
    "                            for element in track_output: \n",
    "                                multiple_values.append(float(element.split()[3]))\n",
    "                            average_value = sum(multiple_values)/len(multiple_values) \n",
    "                            row.append(average_value)\n",
    "                        elif tracksColFile_dict[trackname][0] == 'binary': \n",
    "                            row.append(len(track_output))\n",
    "                        else: \n",
    "                            row.append(((str(site)+\"ERROR_track_coloumns_not_4_or_5_or_binary_\"+trackname)))\n",
    "                            \n",
    "\n",
    "        #sequence stuff                \n",
    "        for distance in list_of_surrounding_contexts: \n",
    "            seq_around = str(alignment[0,site[1]-1:site[1]+2].seq)\n",
    "            if seq_around != '': \n",
    "                seq_around = str(alignment[0,site[1]-distance:site[1]+distance+1].seq)\n",
    "                Acount = seq_around.count('a')+seq_around.count(\"A\")\n",
    "                Gcount = seq_around.count('g')+seq_around.count(\"G\")\n",
    "                Ccount = seq_around.count('c')+seq_around.count(\"C\")\n",
    "                Tcount = seq_around.count('t')+seq_around.count(\"T\")\n",
    "                Apercent = Acount/len(seq_around)\n",
    "                Gpercent = Gcount/len(seq_around)\n",
    "                Cpercent = Ccount/len(seq_around)\n",
    "                Tpercent = Tcount/len(seq_around)\n",
    "                row.extend([Apercent, Gpercent, Cpercent, Tpercent])\n",
    "            else: \n",
    "                row.extend(['NA','NA','NA','NA'])\n",
    "                list_no_seq_at_site.append(site)\n",
    "\n",
    "    row_string = str()\n",
    "    for i in range(0,len(row)): \n",
    "        row_string = row_string+str(row[i])+\"\\t\"\n",
    "    row_string = row_string.rstrip(\"\\t\") # dont need to add the \"\\n\" here as it is added below int he f.write \n",
    "    #     row_string = row_string+\"\\n\" \n",
    "    return(row_string)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a42e91a-93af-4f15-850b-2bddb80dc1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liver starting big loop\n"
     ]
    }
   ],
   "source": [
    "#WRITE THE MODEL #\n",
    "filename_df = tmp_file_dir+'data/{a}/dataframes/{m}/predictorDf{d}.txt'.format(a=tissue,m=model_name,d=model_desc)\n",
    "start_time = time.time()\n",
    "print(tissue,\"starting big loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2aa65-0b19-48ba-8d1f-3e15d4b30f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rowString_handler():\n",
    "    p = multiprocessing.Pool(10)\n",
    "    with open(filename_df, 'a') as f:\n",
    "        for result in p.imap(predictor_rowString, sites):\n",
    "            f.write('%s\\n' % result)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    rowString_handler()\n",
    "\n",
    "error_log += ((\"creating the df loop took \"+str(time.time()-start_time)[0:4]+\" seconds\\n\"))\n",
    "\n",
    "#writing error log to file \n",
    "with open(tmp_file_dir+\"data/{a}/dataframes/{m}/predictorDf{d}_errorlog.txt\".format(a=tissue,m=model_name,d=model_desc),\"w\") as f: \n",
    "      f.write(error_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e08bd23-fb57-4ad2-9919-9a8b60c66d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd6b762-3dc9-42f0-a9cd-18f19aae9316",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd ../../../\n",
    "grep 'buffer' data/skin/dataframes/model9/predictorDf.txt >> data/skin/dataframes/model9/predictorDf_errorlog.txt\n",
    "grep 'discord' data/skin/dataframes/model9/predictorDf.txt >> data/skin/dataframes/model9/predictorDf_errorlog.txt\n",
    "grep -v 'discord' data/skin/dataframes/model9/predictorDf.txt > data/skin/dataframes/model9/predictorDf_noDiscord.txt\n",
    "grep -v 'buffer' data/skin/dataframes/model9/predictorDf_noDiscord.txt >  data/skin/dataframes/model9/predictorDf.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2a372c7f-6f45-4bd2-89e0-fab9d40eaf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd ../../../data/global/track_data/annotation/\n",
    "sort -k1,1 -k4,4n global_annotation.gff > global_annotation_sorted.gff\n",
    "bgzip global_annotation_sorted.gff\n",
    "tabix -p gff global_annotation_sorted.gff.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "101156e3-346a-4687-ad84-37a1d39d38ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd ../../../\n",
    "grep 'buffer' data/blood/dataframes/model7/predictorDf_allTissueSpecTracks.txt >> data/blood/dataframes/model7/predictorDf_allTissueSpecTracks_errorlog.txt\n",
    "grep 'discord' data/blood/dataframes/model7/predictorDf_allTissueSpecTracks.txt >> data/blood/dataframes/model7/predictorDf_allTissueSpecTracks_errorlog.txt\n",
    "grep -v 'discord' data/blood/dataframes/model7/predictorDf_allTissueSpecTracks.txt > data/blood/dataframes/model7/predictorDf_allTissueSpecTracks_noDiscord.txt\n",
    "grep -v 'buffer' data/blood/dataframes/model7/predictorDf_allTissueSpecTracks_noDiscord.txt > data/blood/dataframes/model7/predictorDf_allTissueSpecTracks_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b590acb-7542-4411-8504-f958eec1eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "grep \"buffer\" ../../../data/liver/dataframes/model4/predictorDf.txt >> ../../../data/liver/dataframes/model4/predictorDf_discordantBuffer.txt\n",
    "grep \"discord\" ../../../data/liver/dataframes/model4/predictorDf.txt >> ../../../data/liver/dataframes/model4/predictorDf_discordantBuffer.txt\n",
    "grep -v \"discord\" ../../../data/liver/dataframes/model4/predictorDf.txt >  ../../../data/liver/dataframes/model4/predictorDf_noDiscord.txt\n",
    "grep -v \"buffer\" ../../../data/liver/dataframes/model4/predictorDf_noDiscord.txt >  ../../../data/liver/dataframes/model4/predictorDf.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbef5c2-3c16-4668-ba5b-11ef8a16d0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28ab0e69-3daa-4d1a-b985-6e52b94e338b",
   "metadata": {},
   "source": [
    "# extract the code from the loop and paste here so can trouble shoot directly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "032f300a-1f0f-43e1-ac81-c2f851d3fb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W::hts_idx_load3] The index file is older than the data file: ../../../data/liver/track_data/H3k27ac/H3k27ac.bed.gz.tbi\n",
      "[W::hts_idx_load3] The index file is older than the data file: ../../../data/liver/track_data/H3k27ac/H3k27ac.bed.gz.tbi\n",
      "[W::hts_idx_load3] The index file is older than the data file: ../../../data/liver/track_data/H3k27ac/H3k27ac.bed.gz.tbi\n",
      "[W::hts_idx_load3] The index file is older than the data file: ../../../data/liver/track_data/H3k27ac/H3k27ac.bed.gz.tbi\n",
      "[W::hts_idx_load3] The index file is older than the data file: ../../../data/liver/track_data/H3k27ac/H3k27ac.bed.gz.tbi\n",
      "[W::hts_idx_load3] The index file is older than the data file: ../../../data/liver/track_data/H3k27ac/H3k27ac.bed.gz.tbi\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not create iterator for region 'chr1:1843628-1843628'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_71494/1735182940.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mdistance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_surrounding_contexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrecord\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpysam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTabixfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m                \u001b[0;31m#if no value at that site\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mNa_is_0_or_NA\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Na=0\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                             \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpysam/libctabix.pyx\u001b[0m in \u001b[0;36mpysam.libctabix.TabixFile.fetch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not create iterator for region 'chr1:1843628-1843628'"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "#big boy function \n",
    "for sit in sites[0:10]: \n",
    "    row = []\n",
    "    if site[1] <= distance_max or site[1]+distance_max >= fastas_dict[site[0]][1]:               # only use sites that will have values for site +- max distance (buffer). second element in fastas dict is the length \n",
    "        row.extend([str(site),\"out of buffer range\"])\n",
    "\n",
    "    else: \n",
    "        row.extend([site[0], site[1]])\n",
    "        alignment = fastas_dict[site[0]][0]                                                       #create the alingment from the list of fastas \n",
    "\n",
    "        #makes the triplet and tests alignment \n",
    "        if site[2] ==1:                                     #muts_by-chrom_dict is literally a dictionary containing list of sites that are mutations in that chrom. \n",
    "            mutation_row = mutations_df[(mutations_df.chromosome == site[0]) & (mutations_df.start == site[1])]  #get the row containing mut info out of the df \n",
    "            old_bp = mutation_row.ref.values[0]\n",
    "            old_triplet = (str(alignment[0,site[1]-1])+str(old_bp)+str(alignment[0,site[1]+1])).upper()\n",
    "            row.extend([old_triplet, 1])#usin the old/ref triplet in the df instead (the mutation happened to the old triplet! )\n",
    "\n",
    "            seq_triplet = str(alignment[0,site[1]-1:site[1]+2].seq)\n",
    "            if old_triplet.upper() != seq_triplet.upper():                                                          #testing that \n",
    "                row.append(\"discordant. triplet using daata = \"+old_triplet+\", seqeunce triplet = \"+seq_triplet)\n",
    "        else: \n",
    "            triplet= str(alignment[0,site[1]-1:site[1]+2].seq).upper()\n",
    "            row.extend([triplet,0])\n",
    "\n",
    "        for trackname,track_val in tracksColFile_dict.items():    \n",
    "            data_col = track_val[0] \n",
    "            global_or_tissue_specific = track_val[1]\n",
    "            Na_is_0_or_NA = track_val[2]\n",
    "            filename = tmp_file_dir+track_val[3]\n",
    "\n",
    "            if trackname == \"annotation\": \n",
    "                if not [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1], site[1]+1)]:                #if no value at that site \n",
    "                     row.append(\"not_transcribed\")\n",
    "                else: \n",
    "                    track_output = [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1], site[1]+1)]\n",
    "                    old_labels = [element.split()[2] for element in track_output]\n",
    "                    converted_list, final_label,alien_labels = [],str(),[] \n",
    "                    for label in old_labels: \n",
    "                        if label in annotation_handling.annotation_conversion.keys():  #even though i though i controlled for it, occasiaonlyl there would eb anew label, so sontrol for this \n",
    "                            converted_list.append(annotation_handling.annotation_conversion[label])\n",
    "                        else: \n",
    "                            alien_labels.append(label)\n",
    "                    final_label = annotation_handling.annotation_priorityLabel(converted_list)\n",
    "                    if len(alien_labels) != 0:    # if there is an \"alien\" annotation label, then just add that into the position and i can handle on ind basis later \n",
    "                        for label in alien_labels: \n",
    "                            final_label+=\"_\"+label\n",
    "                    row.append(final_label)\n",
    "            elif trackname == \"mappability\": \n",
    "                if [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1]-distance, site[1]+distance+1)]: row.append(\"mappable\")\n",
    "                else: row.append(\"not\")\n",
    "            elif trackname in [\"dist_rep_org_main\",\"dist_rep_org_all\"]: \n",
    "                row.append(closest_val.shortest_distance(site,filename))\n",
    "            elif trackname == \"CpGisland\": \n",
    "                if [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1], site[1]+1)]: row.append(\"island\")\n",
    "                elif closest_val.shortest_distance(site,filename) <= 2000: row.append(\"shore\")\n",
    "                else: row.append(\"not\")\n",
    "            elif \"methylation\" in trackname:\n",
    "                #########################################working below \n",
    "                for distance in list_of_surrounding_contexts: \n",
    "                    track_output = [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1]-distance, site[1]+1+distance)]\n",
    "                    if track_output: \n",
    "                        num_reads_list,percent_methyl_list,next_door = [],[],False\n",
    "                        for record in track_output: \n",
    "                            cur_site = int(record.split(\"\\t\")[1])\n",
    "                            if cur_site == site[1]: \n",
    "                                num_reads_at_site,percent_methylated = record.split(\"|\")[6:8]\n",
    "                                if int(num_reads_at_site) >= 3: \n",
    "                                    num_reads_list.append(int(num_reads_at_site))\n",
    "                                    percent_methyl_list.append(float(percent_methylated))\n",
    "                            elif cur_site in [site[1]-1,site[1]+1] and distance in [0,1]: next_door = True\n",
    "                        if num_reads_list: # if the list has eleemtns \n",
    "                            row.extend([np.mean(percent_methyl_list),np.mean(num_reads_list)])\n",
    "                        else: # if the list is empty \n",
    "                            if next_door==True: #if the only reason the result is empty is that there is nothing htere, but tabix grabbed it b/c next door \n",
    "                                row.extend([\"no_percent_data\",\"no_coverage_data\"])\n",
    "                            else: #the result is empty because all the returns had coverage less than 3 \n",
    "                                row.extend([\"infsuff_coverage\",\"infsuff_coverage\"])\n",
    "                #         else: \n",
    "                #             for record in track_output:\n",
    "                #                 num_reads_at_site,percent_methylated = record.split(\"|\")[6:8]\n",
    "                #                 if int(num_reads_at_site) >= 3: \n",
    "                #                     num_reads_list.append(int(num_reads_at_site))\n",
    "                #                     percent_methyl_list.append(float(percent_methylated))\n",
    "                #             row.extend([np.mean(percent_methyl_list),np.mean(num_reads_list)])\n",
    "                    else: \n",
    "                        row.extend([\"no_percent_data\",\"no_coverage_data\"])\n",
    "            ############################################################################working abovce\n",
    "            else: \n",
    "                for distance in list_of_surrounding_contexts: \n",
    "                    if not [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1]-distance, site[1]+distance+1)]:                #if no value at that site \n",
    "                        if Na_is_0_or_NA == \"Na=0\": \n",
    "                            row.append(0)\n",
    "                        else: \n",
    "                            row.extend([\"NA\"])                                                                                      \n",
    "                    else:                                                   \n",
    "                        track_output = [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1]-distance, site[1]+distance+1)]\n",
    "                        multiple_values = []\n",
    "\n",
    "                        if tracksColFile_dict[trackname][0] == 4:\n",
    "                            for element in track_output: \n",
    "                                multiple_values.append(float(element.split()[4]))\n",
    "                            average_value = sum(multiple_values)/len(multiple_values) \n",
    "                            row.append(average_value)\n",
    "                        elif tracksColFile_dict[trackname][0] == 3: \n",
    "                            for element in track_output: \n",
    "                                multiple_values.append(float(element.split()[3]))\n",
    "                            average_value = sum(multiple_values)/len(multiple_values) \n",
    "                            row.append(average_value)\n",
    "                        elif tracksColFile_dict[trackname][0] == 'binary': \n",
    "                            row.append(len(track_output))\n",
    "                        else: \n",
    "                            row.append(((str(site)+\"ERROR_track_coloumns_not_4_or_5_or_binary_\"+trackname)))\n",
    "                            \n",
    "\n",
    "        #sequence stuff                \n",
    "        for distance in list_of_surrounding_contexts: \n",
    "            seq_around = str(alignment[0,site[1]-1:site[1]+2].seq)\n",
    "            if seq_around != '': \n",
    "                seq_around = str(alignment[0,site[1]-distance:site[1]+distance+1].seq)\n",
    "                Acount = seq_around.count('a')+seq_around.count(\"A\")\n",
    "                Gcount = seq_around.count('g')+seq_around.count(\"G\")\n",
    "                Ccount = seq_around.count('c')+seq_around.count(\"C\")\n",
    "                Tcount = seq_around.count('t')+seq_around.count(\"T\")\n",
    "                Apercent = Acount/len(seq_around)\n",
    "                Gpercent = Gcount/len(seq_around)\n",
    "                Cpercent = Ccount/len(seq_around)\n",
    "                Tpercent = Tcount/len(seq_around)\n",
    "                row.extend([Apercent, Gpercent, Cpercent, Tpercent])\n",
    "            else: \n",
    "                row.extend(['NA','NA','NA','NA'])\n",
    "                list_no_seq_at_site.append(site)\n",
    "\n",
    "    row_string = str()\n",
    "    for i in range(0,len(row)): \n",
    "        row_string = row_string+str(row[i])+\"\\t\"\n",
    "    row_string = row_string.rstrip(\"\\t\") # dont need to add the \"\\n\" here as it is added below int he f.write \n",
    "    #     row_string = row_string+\"\\n\" \n",
    "    print(row_string)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b416bda6-6ba2-4094-822d-bd6a63b66dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../data/liver/track_data/H3k27me3/H3k27me3.bed.gz'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc472611-b993-4568-9baa-9323e4984f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zcat ../../../data/liver/track_data/H3k27me3/H3k27me3.bed.gz | head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86c7139e-422d-4df5-9732-bb04e612a0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 7.6G\n",
      "drwxr-xr-x  3 omanmade domain users 4.0K Jul  9 15:26 .\n",
      "drwxr-xr-x 12 omanmade domain users  218 Sep 21  2022 ..\n",
      "-rw-r--r--  1 omanmade domain users    0 Jul  9 15:26 H3k27me3.bed\n",
      "-rw-r--r--  1 omanmade domain users  11M Jul  9 15:26 H3k27me3_unliftgedhg19.bed\n",
      "-rw-r--r--  1 omanmade domain users  52M Jul  9 15:17 H3k27me3_unliftgedhg38.bed\n",
      "-rw-r--r--  1 omanmade domain users 7.5G Jul  9 15:26 H3k27me3_unsorted.bed\n",
      "-rw-r--r--  1 omanmade domain users 221K Jun  4  2009 hg19ToHg18.over.chain.gz\n",
      "-rw-r--r--  1 omanmade domain users  948 Jul  9 14:47 hg19ToHg18_wget_output.txt\n",
      "-rw-r--r--  1 omanmade domain users 1.2M Dec 31  2013 hg38ToHg19.over.chain.gz\n",
      "-rw-r--r--  1 omanmade domain users 2.5K Jul  9 14:47 hg38ToHg19_wget_output.txt\n",
      "drwxr-xr-x  2 omanmade domain users    6 Jul  9 14:43 .ipynb_checkpoints\n",
      "-rw-r--r--  1 omanmade domain users 2.2M Jul  9 14:45 wget_output.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ../../../data/liver/track_data/H3k27me3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d4f299-e4cd-4ead-9445-b74910c6626c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f5d235-e8ff-4857-a5c4-6f752f017c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfb009a-079b-4181-ae90-09392d0f5158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c1e22-32ca-40dc-843e-211c9c216684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd7c131-d3fb-444a-90f8-580b1b3af289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421e50b-252e-4fa4-9897-5db32d7084cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d04468-86f6-4f07-9253-49fb36e3f3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc357c7-9ac9-44a3-a7b3-2c4e9b5e2011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc60aec-1108-475d-a695-6fe44baf5582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e4490-8efa-41c0-bf83-e26095dc9081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb556c6-9691-494d-866b-5afc699a834e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a0ea4-085e-4063-b9a2-f2fcb1357fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bd4d67-d496-43ef-9f37-aa025c1c1d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74421c04-d062-4f86-a63a-afb6c79b893f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59cb41c6-b364-4a99-8acf-b41074ae49d9",
   "metadata": {},
   "source": [
    "# scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d1fcd59-a785-4a5b-b298-24843a6d0e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  29412 54\n"
     ]
    }
   ],
   "source": [
    "! awk '{print NF}' ../../../data/liver/dataframes/model9/predictorDf.txt | sort | uniq -c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "543d3cb6-f612-4298-bfff-febbffe0c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_blood = open(\"../../../data/blood/dataframes/model9/predictorDf.txt\").readlines()[0].split()\n",
    "header_liver = open(\"../../../data/liver/dataframes/model9/predictorDf.txt\").readlines()[0].split()\n",
    "header_skin = open(\"../../../data/skin/dataframes/model9/predictorDf.txt\").readlines()[0].split()\n",
    "header_germline = open(\"../../../data/germline/dataframes/model9/predictorDf.txt\").readlines()[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7efce41-23e7-4584-9bde-15b1fd9b2ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 54, 57, 60)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(header_blood), len(header_liver), len(header_germline), len(header_skin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75928edc-dacc-46bc-89f4-78566ff2f91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H3k27me3-0\n",
      "H3k27me3-100\n",
      "H3k27me3-10000\n"
     ]
    }
   ],
   "source": [
    "for colname in header_germline: \n",
    "    if colname not in header_liver: \n",
    "        print(colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "586735b0-0318-42d4-bfb8-c2974b2e72af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chr1', 248306415, 0]\tout of buffer range\n",
      "['chr1', 248883641, 0]\tout of buffer range\n",
      "['chr1', 247425269, 0]\tout of buffer range\n",
      "['chr1', 248149009, 0]\tout of buffer range\n",
      "['chr5', 180910052, 0]\tout of buffer range\n",
      "['chr5', 965, 0]\tout of buffer range\n",
      "['chr5', 181494540, 0]\tout of buffer range\n",
      "['chr5', 181063363, 0]\tout of buffer range\n",
      "['chr5', 181528543, 0]\tout of buffer range\n",
      "['chr7', 159206929, 0]\tout of buffer range\n"
     ]
    }
   ],
   "source": [
    "! awk 'NF==7{print $0}' ../../../data/skin/dataframes/model9/predictorDf.txt | head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6f3d4ed-6c00-4c27-bf67-9a9ffcf24d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# imports ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from numpy.random import choice\n",
    "import collections\n",
    "from Bio import AlignIO\n",
    "import pysam \n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import multiprocessing\n",
    "import sys \n",
    "import json \n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d20e6f9f-cdb7-48b6-9248-4b6943256fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_tmp =\"../../../\"+\"data/global/sequence/chr1.fa.gz\"\n",
    "with gzip.open(filename_tmp, \"rt\") as handle:\n",
    "    alignment = (AlignIO.read(handle,\"fasta\"))\n",
    "\n",
    "sites=[]\n",
    "tmp_file_dir = \"../../../\"\n",
    "#get chrom length information so I can perform weighted choice for non-mut site selection\"\n",
    "ChromLengths = pd.read_csv(tmp_file_dir+'data/global/sequence/hg38_chromosomelengths.csv') #read in the csv file of hg38 chrom lengths I found on the internets \n",
    "total_length=0 #lets sum (get the total length) \n",
    "for length in list(ChromLengths.Length): \n",
    "    total_length+=int(length.replace(\",\",\"\"))\n",
    "\n",
    "#build dictionary to store porbability \n",
    "dict_lengths = {}#creat emepty dictionary \n",
    "for x in range (0,22): \n",
    "        tmp_index = x +1\n",
    "        length = str(ChromLengths[x:x+1]).split()[4]\n",
    "        length = length.replace(\",\", \"\")\n",
    "        length = int(length)\n",
    "        dict_lengths[\"chr\"+str(tmp_index)] = length\n",
    "\n",
    "#make the porbability of choosing a chrom based on length \n",
    "list_chroms = ['chr' + str(i) for i in range(1, 23)]\n",
    "list_chrom_probabilities = []\n",
    "for chrom in list_chroms: \n",
    "    list_chrom_probabilities.append(dict_lengths[chrom]/total_length)\n",
    "list_chrom_probabilities[0] = list_chrom_probabilities[0]+1-sum(list_chrom_probabilities) # adds the 0.00000001 left from rounding errors to the chr1 so sum adds perfectly to 1. \n",
    "assert(sum(list_chrom_probabilities)==1)\n",
    "\n",
    "#perfrom the non-mutant site draw \n",
    "number_nonmuts = 10000\n",
    "chrom_draw = choice(list_chroms, number_nonmuts,p=list_chrom_probabilities)\n",
    "\n",
    "# print(tissue,\"make the sites list with the chr# and site\" )\n",
    "for i in (range(1,23)): \n",
    "    chrom = \"chr\"+str(i)\n",
    "    chrom_nchoose = list(chrom_draw).count(\"chr\"+str(i))\n",
    "    chrom_sites_chosen = random.sample(range(1, dict_lengths[chrom]), chrom_nchoose) #without duplucates \n",
    "    for j in chrom_sites_chosen: \n",
    "        sites.append([chrom,j,0])# the 0 if for the mutation status column. 0 = no "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "278b5a25-7fb7-46e9-9e95-9cd6f0dbc2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# site = ['chr1',1212518]\n",
    "filename=\"../../../data/germline/track_data/methylation/methylation.bed.gz\"\n",
    "list_of_surrounding_contexts=[0]\n",
    "\n",
    "list_rows =[]\n",
    "# for site in (sites[0:880]): \n",
    "site = [\"chr14\",21919600]\n",
    "row = [site[0],site[1],alignment[0][site[1]].upper() ]\n",
    "\n",
    "#     track_output =['chr2\\t91661791\\t91661792\\t\"0\"|1|-|93087629|93087630|0,255,0|3|0|CCT|CCT|1',\n",
    "#      'chr2\\t91661791\\t91661792\\t\"0\"|1|-|93740591|93740592|0,255,0|1|100|CCT|CCT|1',\n",
    "#      'chr2\\t91661791\\t91661792\\t\"0\"|1|-|94142516|94142517|0,255,0|1|0|CCT|CCT|3',\n",
    "#      'chr2\\t91661791\\t91661792\\t\"0\"|4|-|4393|4394|0,255,0|4|0|CCT|CCT|2']\n",
    "\n",
    "#########################################working below \n",
    "for distance in list_of_surrounding_contexts: \n",
    "    track_output = [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1]-distance, site[1]+1+distance)]\n",
    "    if track_output: \n",
    "        num_reads_list,percent_methyl_list,next_door = [],[],False\n",
    "        for record in track_output: \n",
    "            cur_site = int(record.split(\"\\t\")[1])\n",
    "            if cur_site == site[1]: \n",
    "                num_reads_at_site,percent_methylated = record.split(\"|\")[6:8]\n",
    "                if int(num_reads_at_site) >= 3: \n",
    "                    num_reads_list.append(int(num_reads_at_site))\n",
    "                    percent_methyl_list.append(float(percent_methylated))\n",
    "            elif cur_site in [site[1]-1,site[1]+1] and distance in [0,1]: next_door = True\n",
    "        if num_reads_list: # if the list has eleemtns \n",
    "            row.extend([np.mean(percent_methyl_list),np.mean(num_reads_list)])\n",
    "        else: # if the list is empty \n",
    "            if next_door==True: #if the only reason the result is empty is that there is nothing htere, but tabix grabbed it b/c next door \n",
    "                row.extend([\"no_percent_data\",\"no_coverage_data\"])\n",
    "            else: #the result is empty because all the returns had coverage less than 3 \n",
    "                row.extend([\"infsuff_coverage\",\"infsuff_coverage\"])\n",
    "#         else: \n",
    "#             for record in track_output:\n",
    "#                 num_reads_at_site,percent_methylated = record.split(\"|\")[6:8]\n",
    "#                 if int(num_reads_at_site) >= 3: \n",
    "#                     num_reads_list.append(int(num_reads_at_site))\n",
    "#                     percent_methyl_list.append(float(percent_methylated))\n",
    "#             row.extend([np.mean(percent_methyl_list),np.mean(num_reads_list)])\n",
    "    else: \n",
    "        row.extend([\"no_percent_data\",\"no_coverage_data\"])\n",
    "list_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e80ddc1-462b-49e1-b0d3-235ae2b84979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chr14', 21919600, 'T', 'infsuff_coverage', 'infsuff_coverage']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cad19dd1-2a49-4e35-9177-66affa5f7cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chr1', 3008881, 'A', 'no_percent_data', 'no_coverage_data']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-mutability] *",
   "language": "python",
   "name": "conda-env-.conda-mutability-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
