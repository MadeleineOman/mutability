{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "596defd2-d6a0-4b4f-a508-c4454e26534f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fill each chromosome's empty list  with the sites for that chrom \n",
      "make the sites list with the chr# and site\n",
      "making the fastas dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:50<00:00,  2.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# imports ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from numpy.random import choice\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from Bio import AlignIO\n",
    "import pysam \n",
    "from tqdm import tqdm \n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import multiprocessing\n",
    "import sys \n",
    "sys.path.append('/research/projects/hsapiens/mutability/analysis/global/track_data/annotation/') \n",
    "import annotation_handling\n",
    "import json \n",
    "\n",
    "\n",
    "\n",
    "# command line input ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# tissue = sys.argv[1]\n",
    "tissue = \"liver\"\n",
    "# model_desc = sys.argv[2]\n",
    "model_desc = \"model2\"\n",
    "# list_of_surrounding_contexts = json.loads(sys.argv[3]) #note if you chnagee/ increase this, then you oncrese the buffer zone (not using sites in the buffer, len(dna)-max_distance )\n",
    "list_of_surrounding_contexts = [1,100,10000]\n",
    "\n",
    "if tissue == \"germline\": \n",
    "    mutations_lines = open('../../../data/germline/mutation_data/mutations_hg18_final.bed').readlines()\n",
    "    mutations_df = pd.read_table('../../../data/germline/mutation_data/mutations_hg18_final.bed',sep=\"\\t\",header = None)\n",
    "    mutations_df.columns = [\"chromosome\",\"start\",\"fake_end\",\"ref\",\"alt\",\"Fathers_age_at_conception\",\"Mothers_age_at_conception\"]\n",
    "elif tissue in [\"blood\",\"liver\"]:\n",
    "    mutations_lines = open('../../../data/{t}/mutations/mutations.bed'.format(t=tissue)).readlines()\n",
    "    mutations_df = pd.read_table('../../../data/{t}/mutations/mutations.bed'.format(t=tissue),sep=\"\\t\",header = None)\n",
    "    mutations_df.columns = [\"chromosome\",\"start\",\"fake_end\",\"ref\",\"alt\",\"ID\",\"VAF\",\"Gene name\", \"Region\", \"AA\", \"COSMIC\", \"Species\", \"Gender\", \"Age_in_years\",           \n",
    "                            \"Tissue/Cell type\",\"Single-cell_genomics_biotechnology_or_Method\",\"Control_sample_or_tissue\"]\n",
    "else: \n",
    "    print(\"tissue specified not yet supported\")\n",
    "\n",
    "    \n",
    "\n",
    "#dictionry where i specify which col contains the information in the datafile , 0 indexed \n",
    "tracksColFile_dict = json.load(open(\"../../../data/{t}/objects/{m}/tracksColDict.txt\".format(m=model_desc,t=tissue)))#  \n",
    "\n",
    "\n",
    "#mutant sites ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "#create a dictionary where each chrom key will have a n empty list \n",
    "duplicate_lines = []\n",
    "muts_bychrom_dict = {}\n",
    "for x in range(1,23): \n",
    "    key_string = 'chr{n}'.format(n=x)\n",
    "    muts_bychrom_dict[key_string] = []\n",
    "    \n",
    "print(\"fill each chromosome's empty list  with the sites for that chrom \")\n",
    "non_chrnMuts = []#create list of chrom names that dont belong to chrN format --> disgnostic \n",
    "for line in (mutations_lines[1:]): \n",
    "    if line[0]==\"c\":                                               #aking sure the line is a chrN (lots of weird junk..) \n",
    "        chrom_mut = line.split(\"\\t\")[0]\n",
    "        mut_startSite = line.split(\"\\t\")[1]                  #getting rid of the weird double(hgopefully) \n",
    "        if chrom_mut in muts_bychrom_dict.keys():                  #controlling for chrX/chrY\n",
    "            if mut_startSite not in muts_bychrom_dict[chrom_mut]: \n",
    "                muts_bychrom_dict[chrom_mut].append(mut_startSite)\n",
    "            else: duplicate_lines.append(line)\n",
    "        else: \n",
    "            non_chrnMuts.append(chrom_mut)\n",
    "\n",
    "#testing making usre the only sites that dont make it are sex chromosome mutations \n",
    "error_log = str()\n",
    "error_log+=(str(len(non_chrnMuts))+\"  non chrN muts (ommited) from these lables: \"+str(list(np.unique(non_chrnMuts)))+\"\\n\")\n",
    "\n",
    "#add the sites infro from file \n",
    "sites = []#sites = list of sites \n",
    "for chrom_key in muts_bychrom_dict.keys(): \n",
    "    for mutation_element in muts_bychrom_dict[chrom_key]: \n",
    "        sites.append([chrom_key, int(mutation_element),1]) #the 1 is for mutation status column. 1 = yes \n",
    "error_log+=(\"number included mutations = \"+str(len(sites))+\"\\n\")\n",
    "\n",
    "\n",
    "#non -mutant sites ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "#get chrom length information so I can perform weighted choice for non-mut site selection \n",
    "ChromLengths = pd.read_csv('../../../data/global/sequence/hg38_chromosomelengths.csv') #read in the csv file of hg38 chrom lengths I found on the internets \n",
    "total_length=0 #lets sum (get the total length) \n",
    "for length in list(ChromLengths.Length): \n",
    "    total_length+=int(length.replace(\",\",\"\"))\n",
    "\n",
    "#build dictionary to store porbability \n",
    "dict_lengths = {}#creat emepty dictionary \n",
    "for x in range (0,22): \n",
    "        tmp_index = x +1\n",
    "        length = str(ChromLengths[x:x+1]).split()[4]\n",
    "        length = length.replace(\",\", \"\")\n",
    "        length = int(length)\n",
    "        dict_lengths[\"chr\"+str(tmp_index)] = length\n",
    "\n",
    "#make the porbability of choosing a chrom based on length \n",
    "list_chroms = ['chr' + str(i) for i in range(1, 23)]\n",
    "list_chrom_probabilities = []\n",
    "for chrom in list_chroms: \n",
    "    list_chrom_probabilities.append(dict_lengths[chrom]/total_length)\n",
    "list_chrom_probabilities[0] = list_chrom_probabilities[0]+1-sum(list_chrom_probabilities) # adds the 0.00000001 left from rounding errors to the chr1 so sum adds perfectly to 1. \n",
    "assert(sum(list_chrom_probabilities)==1)\n",
    "\n",
    "#perfrom the non-mutant site draw \n",
    "number_nonmuts = len(sites)\n",
    "chrom_draw = choice(list_chroms, number_nonmuts,p=list_chrom_probabilities)\n",
    "\n",
    "print(\"make the sites list with the chr# and site\" )\n",
    "for i in (range(1,23)): \n",
    "    chrom = \"chr\"+str(i)\n",
    "    chrom_nchoose = list(chrom_draw).count(\"chr\"+str(i))\n",
    "    chrom_sites_chosen = random.sample(range(1, dict_lengths[chrom]), chrom_nchoose) #without duplucates \n",
    "    for j in chrom_sites_chosen: \n",
    "        sites.append([chrom,j,0])# the 0 if for the mutation status column. 0 = no \n",
    "\n",
    "        \n",
    "# genral declarations before the big function ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "distance_max = max(list_of_surrounding_contexts)\n",
    "\n",
    "fastas_dict = {}   # creating dictionary with fasta alignment, length of seq, \n",
    "print(\"making the fastas dictionary\")\n",
    "for chrom in tqdm(list_chroms):\n",
    "    filename_tmp = \"../../../data/global/sequence/{c}.fa.gz\".format(c=chrom)\n",
    "    fastas_dict[chrom] = []\n",
    "    with gzip.open(filename_tmp, \"rt\") as handle:\n",
    "        fastas_dict[chrom].append(AlignIO.read(handle,\"fasta\"))\n",
    "        alignment_tmp = fastas_dict[chrom][0]\n",
    "        fastas_dict[chrom].append(len(str(alignment_tmp[0].seq)))\n",
    "\n",
    "                 \n",
    "#generate header ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~        \n",
    "\n",
    "header = \"Chromosome\"+ \"\\t\"+\"site\"+\"\\t\" +\"triplet\"+\"\\t\"+\"mutation_status\"                        # creating the begining of the header \n",
    "for trackname in tracksColFile_dict.keys():   # the rest of the header is a function of tracks \n",
    "    if trackname == \"annotation\": \n",
    "        header = header + \"\\t\"+str(trackname)\n",
    "    else: \n",
    "        for distance in list_of_surrounding_contexts:                                                # and distance (need a col for every track and for every distance value within ) \n",
    "            header = header + \"\\t\"+str(trackname)+\"-\"+str(distance)\n",
    "for distance in list_of_surrounding_contexts:                                                    # creating the end of the header assoc with no track (the seqeunce at different \n",
    "    header = header + \"\\t\"+\"Apercent-\"+str(distance)+ \"\\t\"+\"Gpercent-\"+str(distance)+ \"\\t\"+\"Cpercent-\"+str(distance)+ \"\\t\"+\"Tpercent-\"+str(distance)   # distace values) \n",
    "header = header +\"\\n\"                                                                            # obviously needs to end with a \\n \n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y/%m/%d\").replace(\"/\", \"_\").replace(\":\", \"_\")\n",
    "filename = '../../../data/{a}/dataframes/{m}/predictorDf_{t}.txt'.format(a=tissue,t=timestamp,m=model_desc)\n",
    "with open(filename,\"w\") as f: \n",
    "    f.write(header)       \n",
    "    \n",
    "\n",
    "\n",
    "# big function ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "def predictor_rowString(site): \n",
    "# site = sites[52429]\n",
    "    row = []\n",
    "    if site[1] <= distance_max or site[1]+distance_max >= fastas_dict[site[0]][1]:               # only use sites that will have values for site +- max distance (buffer). second element in fastas dict is the length \n",
    "        row.extend([str(site),\"out of buffer range\"])\n",
    "\n",
    "\n",
    "    else: \n",
    "        row.extend([site[0], site[1]])\n",
    "        alignment = fastas_dict[site[0]][0]                                                       #create the alingment from the list of fastas \n",
    "\n",
    "        #makes the triplet and tests alignment \n",
    "        if site[2] ==1:                                     #muts_by-chrom_dict is literally a dictionary containing list of sites that are mutations in that chrom. \n",
    "            mutation_row = mutations_df[(mutations_df.chromosome == site[0]) & (mutations_df.start == site[1])]  #get the row containing mut info out of the df \n",
    "            old_bp = mutation_row.ref.values[0]\n",
    "            old_triplet = (str(alignment[0,site[1]-1])+str(old_bp)+str(alignment[0,site[1]+1])).upper()\n",
    "            row.extend([old_triplet, 1])#usin the old/ref triplet in the df instead (the mutation happened to the old triplet! )\n",
    "\n",
    "            seq_triplet = str(alignment[0,site[1]-1:site[1]+2].seq)\n",
    "            if old_triplet.upper() != seq_triplet.upper():                                                          #testing that \n",
    "                row.append(\"discordant. triplet using daata = \"+old_triplet+\", seqeunce triplet = \"+seq_triplet)\n",
    "        else: \n",
    "            triplet= str(alignment[0,site[1]-1:site[1]+2].seq).upper()\n",
    "            row.extend([triplet,0])\n",
    "\n",
    "        for trackname,track_val in tracksColFile_dict.items():                 \n",
    "            data_col = track_val[0] \n",
    "            global_or_tissue_specific = track_val[1]\n",
    "            Na_is_0_or_NA = track_val[2]\n",
    "            filename = track_val[3]\n",
    "\n",
    "            if trackname == \"annotation\": \n",
    "                if not [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1]-distance, site[1]+distance+1)]:                #if no value at that site \n",
    "                     row.append(\"not_transcribed\")\n",
    "                else: \n",
    "                    track_output = [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1]-distance, site[1]+distance+1)]\n",
    "                    old_labels = [element.split()[2] for element in track_output]\n",
    "                    converted_list, final_label,alien_labels = [],str(),[] \n",
    "                    for label in old_labels: \n",
    "                        if label in annotation_handling.annotation_conversion.keys():  #even though i though i controlled for it, occasiaonlyl there would eb anew label, so sontrol for this \n",
    "                            converted_list.append(annotation_handling.annotation_conversion[label])\n",
    "                        else: \n",
    "                            alien_labels.append(label)\n",
    "                    final_label = annotation_handling.annotation_priorityLabel(converted_list)\n",
    "                    if len(alien_labels) != 0:    # if there is an \"alien\" annotation label, then just add that into the position and i can handle on ind basis later \n",
    "                        for label in alien_labels: \n",
    "                            final_label+=\"_\"+label\n",
    "                    row.append(final_label)\n",
    "\n",
    "            else: \n",
    "                for distance in list_of_surrounding_contexts: \n",
    "                    if not [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1]-distance, site[1]+distance+1)]:                #if no value at that site \n",
    "                        if Na_is_0_or_NA == \"Na=0\": \n",
    "                            row.append(0)\n",
    "                        else: \n",
    "                            row.extend([\"NA\"])                                                                                      \n",
    "                    else:                                                   \n",
    "                        track_output = [record for record in pysam.Tabixfile(filename).fetch(site[0], site[1]-distance, site[1]+distance+1)]\n",
    "                        multiple_values = []\n",
    "\n",
    "                        if tracksColFile_dict[trackname][0] == 4:\n",
    "                            for element in track_output: \n",
    "                                multiple_values.append(float(element.split()[4]))\n",
    "                            average_value = sum(multiple_values)/len(multiple_values) \n",
    "                            row.append(average_value)\n",
    "                        elif tracksColFile_dict[trackname][0] == 3: \n",
    "                            for element in track_output: \n",
    "                                multiple_values.append(float(element.split()[3]))\n",
    "                            average_value = sum(multiple_values)/len(multiple_values) \n",
    "                            row.append(average_value)\n",
    "                        elif tracksColFile_dict[trackname][0] == 'binary': \n",
    "                            row.append(len(track_output))\n",
    "                        else: \n",
    "                            error_log += ((str(site)+\" ERROR: track coloumns not 4 or 5 or binary: \"+trackname+\"\\n\"))\n",
    "\n",
    "        #sequence stuff                \n",
    "        for distance in list_of_surrounding_contexts: \n",
    "            seq_around = str(alignment[0,site[1]-1:site[1]+2].seq)\n",
    "            if seq_around != '': \n",
    "                seq_around = str(alignment[0,site[1]-distance:site[1]+distance+1].seq)\n",
    "                Acount = seq_around.count('a')+seq_around.count(\"A\")\n",
    "                Gcount = seq_around.count('g')+seq_around.count(\"G\")\n",
    "                Ccount = seq_around.count('c')+seq_around.count(\"C\")\n",
    "                Tcount = seq_around.count('t')+seq_around.count(\"T\")\n",
    "                Apercent = Acount/len(seq_around)\n",
    "                Gpercent = Gcount/len(seq_around)\n",
    "                Cpercent = Ccount/len(seq_around)\n",
    "                Tpercent = Tcount/len(seq_around)\n",
    "                row.extend([Apercent, Gpercent, Cpercent, Tpercent])\n",
    "            else: \n",
    "                row.extend(['NA','NA','NA','NA'])\n",
    "                list_no_seq_at_site.append(site)\n",
    "\n",
    "    row_string = str()\n",
    "    for i in range(0,len(row)): \n",
    "        row_string = row_string+str(row[i])+\"\\t\"\n",
    "    row_string = row_string.rstrip(\"\\t\") # dont need to add the \"\\n\" here as it is added below int he f.write \n",
    "#     row_string = row_string+\"\\n\" \n",
    "    return row_string\n",
    "\n",
    "\n",
    "#WRITE THE MODEL #\n",
    "\n",
    "def rowString_handler():\n",
    "    p = multiprocessing.Pool(10)\n",
    "    with open(filename, 'a') as f:\n",
    "        for result in p.imap(predictor_rowString, sites):\n",
    "            f.write('%s\\n' % result)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    rowString_handler()\n",
    "\n",
    "#writing error log to file \n",
    "with open(\"../../../data/{a}/dataframes/{m}/predictorDf_{t}_errorlog.txt\".format(a=tissue,t=timestamp,m=model_desc),\"w\") as f: \n",
    "      f.write(error_log)\n",
    "     \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b590acb-7542-4411-8504-f958eec1eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "grep \"buffer\" ../../../data/liver/dataframes/model2/predictorDf_2022_05_18.txt > ../../../data/liver/dataframes/model2/predictorDf_discordantBuffer.txt\n",
    "grep \"discord\" ../../../data/liver/dataframes/model2/predictorDf_2022_05_18.txt >> ../../../data/liver/dataframes/model2/predictorDf_discordantBuffer.txt\n",
    "grep -v \"discord\" ../../../data/liver/dataframes/model2/predictorDf_2022_05_18.txt >  ../../../data/liver/dataframes/model2/predictorDf_noDiscord.txt\n",
    "grep -v \"buffer\" ../../../data/liver/dataframes/model2/predictorDf_noDiscord.txt >  ../../../data/liver/dataframes/model2/predictorDf.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-mutability] *",
   "language": "python",
   "name": "conda-env-.conda-mutability-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
